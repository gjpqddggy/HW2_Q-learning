{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.autograd as autograd\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# from tensorboardX import SummaryWriter \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "# register(\n",
    "#     id='FrozenLakeNotSlippery-v0',\n",
    "#     entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "#     kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "#     max_episode_steps=100,\n",
    "#     reward_threshold=0.78, # optimum = .8196\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_id = 'FrozenLakeNotSlippery-v0'\n",
    "env = gym.make(env_id)\n",
    "\n",
    "\"\"\"\n",
    "use gym (openAI)\n",
    "https://blog.techbridge.cc/2017/11/04/openai-gym-intro-and-q-learning/\n",
    "\n",
    "\"\"\"\n",
    "print(env.observation_space.n)\n",
    "print(env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# epsilon greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 3000.\n",
    "\n",
    "def epsilon_by_frame(frame_idx):\n",
    "    \"\"\"\n",
    "    your design\n",
    "    \"\"\"\n",
    "    epsilon = max(math.exp(-(1/epsilon_decay)*frame_idx), epsilon_final)\n",
    "#     epsilon = math.exp(-(1/epsilon_decay)*frame_idx)\n",
    "\n",
    "    return epsilon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(100000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(state, epsilon):\n",
    "    if random.random() > epsilon:\n",
    "        action = 0\n",
    "        for i in range(4):\n",
    "            if Q[state][i] > Q[state][action]: action = i\n",
    "    else:\n",
    "        action = np.random.choice([0,1,2,3])\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q = np.zeros((16,4))\n",
    "total_rewards = 0\n",
    "all_rewards    = []\n",
    "frames = []\n",
    "frames_count = 0\n",
    "episode_reward = 0\n",
    "episode_count = 0\n",
    "num_frames = 100000\n",
    "gamma = 0.8\n",
    "rate = 0.9\n",
    "count = 0\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    # get epsilon\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    \n",
    "    # forward\n",
    "    action  = act(state, epsilon) \n",
    "\n",
    "    # interact with environment\n",
    "    env.render()\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    \n",
    "    # update Q table\n",
    "    Q[state][action] = Q[state][action] + rate*(reward + gamma*max(Q[next_state]) - Q[state][action])\n",
    "    \n",
    "    # go to next state\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    frames_count += 1\n",
    "    \n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        episode_count += 1\n",
    "        total_rewards += episode_reward\n",
    "        all_rewards.append(total_rewards/episode_count)\n",
    "        frames.append(frames_count)\n",
    "        frames_count = 0\n",
    "        episode_reward = 0\n",
    "        if (reward == 1): count += 1\n",
    "        print('-----------done')\n",
    "\n",
    "env.close()\n",
    "print(episode_count)\n",
    "print(count)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([all_rewards[i] for i in range(episode_count)])\n",
    "# print(len(all_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([frames[i] for i in range(episode_count)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
